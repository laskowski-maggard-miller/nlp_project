{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4e1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "import acquire as ac\n",
    "import prepare as pr\n",
    "import wrangle as wr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1797d06",
   "metadata": {},
   "source": [
    "# Acquiring Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a295cda7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 60 rows with empty Readmes.\n",
      "Removed 197 rows with Readmes < 10 words long.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_analysis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/lzs3plw14ms00nxw2vwq68vc0000gn/T/ipykernel_65596/2906993161.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mshared_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'use'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'using'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'run'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_languages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat_wrangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshared_word_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/nlp_project/wrangle.py\u001b[0m in \u001b[0;36mcat_wrangle\u001b[0;34m(extra_words, exclude_words)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_analysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'language_group'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Creates X and y versions of train, test and split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_analysis' is not defined"
     ]
    }
   ],
   "source": [
    "shared_word_list = ['cat', 'use', 'image', 'using', 'file', 'run']\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test, df, df_languages = wr.cat_wrangle(extra_words = (shared_word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abf866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = wr.splitter(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1590f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46639cc",
   "metadata": {},
   "source": [
    "# Preparing Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dce0c",
   "metadata": {},
   "source": [
    "## Cleaning the original DataFrame: \n",
    "- Creates rows for repo name, top language, the contents of the readme, and the cleaned text of the readmes:\n",
    "    - Removes newlines, urls, and words that are longer than 14 characters.\n",
    "    - Makes it all lowercase, tokenizes the words, and then lemmatizes (or stems) them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3550d",
   "metadata": {},
   "source": [
    "#### Removing any records that are fewer than 11 words (leaves 743 records left):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b87df2",
   "metadata": {},
   "source": [
    "# Exploration\n",
    "- My focus is on bigrams and word visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d77a30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.language_group.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989aaf05",
   "metadata": {},
   "source": [
    "## Creating WordClouds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b01b8",
   "metadata": {},
   "source": [
    "### Python:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0245cf8",
   "metadata": {},
   "source": [
    "#### Creating separate DataFrame of Python-lead READMEs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6691b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_df = train[train.language_group == 'Python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62285016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "python_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0122c7",
   "metadata": {},
   "source": [
    "#### Joining all the text from Python READMEs into a single group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text = ''\n",
    "for i in range(python_df.shape[0]):\n",
    "    python_text = python_text + python_df.cleaned.iloc[i]\n",
    "    \n",
    "python_text[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435567e",
   "metadata": {},
   "source": [
    "#### Creating a WordCloud from all of the Python text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f3f19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = WordCloud(background_color='white').generate(python_text)\n",
    "# WordCloud() produces an image object, which can be displayed with plt.imshow\n",
    "plt.imshow(img)\n",
    "# axis aren't very useful for a word cloud\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5e6dc",
   "metadata": {},
   "source": [
    "#### Creating bigrams from the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a41c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(python_text.split(), 2)\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_python_bigrams = (pd.Series(nltk.ngrams(python_text.split(), 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_python_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b684c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_python_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943da5e",
   "metadata": {},
   "source": [
    "Given that these bigrams seem to be single letters, mostly nonsensical, it may be worth adding a condition to the cleaning function that eliminates words that are only single letters (although it's worth noting that this could eliminate things like \"I\" or \"a.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b54f7",
   "metadata": {},
   "source": [
    "### Scala:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_df = train[train.language_group == 'Scala']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_text = ''\n",
    "for i in range(scala_df.shape[0]):\n",
    "    scala_text = scala_text + scala_df.cleaned.iloc[i]\n",
    "    \n",
    "scala_text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = WordCloud(background_color='white').generate(scala_text)\n",
    "# WordCloud() produces an image object, which can be displayed with plt.imshow\n",
    "plt.imshow(img)\n",
    "# axis aren't very useful for a word cloud\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a886028",
   "metadata": {},
   "outputs": [],
   "source": [
    "scala_bigrams = nltk.ngrams(scala_text.split(), 2)\n",
    "list(scala_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab2207f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_20_scala_bigrams = (pd.Series(nltk.ngrams(scala_text.split(), 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_scala_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd296b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_scala_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec530422",
   "metadata": {},
   "source": [
    "## JavaScript:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e93488",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_df = train[train.language_group == 'JavaScript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d652f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_text = ''\n",
    "for i in range(js_df.shape[0]):\n",
    "    js_text = js_text + js_df.cleaned.iloc[i]\n",
    "    \n",
    "js_text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3215fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = WordCloud(background_color='white').generate(js_text)\n",
    "# WordCloud() produces an image object, which can be displayed with plt.imshow\n",
    "plt.imshow(img)\n",
    "# axis aren't very useful for a word cloud\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_bigrams = nltk.ngrams(js_text.split(), 2)\n",
    "list(js_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_js_bigrams = (pd.Series(nltk.ngrams(js_text.split(), 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_js_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bca98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_js_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b77e86a",
   "metadata": {},
   "source": [
    "## Other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df = train[train.language_group == 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb02afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_text = ''\n",
    "for i in range(other_df.shape[0]):\n",
    "    other_text = other_text + other_df.cleaned.iloc[i]\n",
    "    \n",
    "other_text[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c58148",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = WordCloud(background_color='white').generate(other_text)\n",
    "# WordCloud() produces an image object, which can be displayed with plt.imshow\n",
    "plt.imshow(img)\n",
    "# axis aren't very useful for a word cloud\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78035dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_bigrams = nltk.ngrams(other_text.split(), 2)\n",
    "list(other_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e73235",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_other_bigrams = (pd.Series(nltk.ngrams(other_text.split(), 2))\n",
    "                      .value_counts()\n",
    "                      .head(20))\n",
    "\n",
    "top_20_other_bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f82730",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {k[0] + ' ' + k[1]: v for k, v in top_20_other_bigrams.to_dict().items()}\n",
    "img = WordCloud(background_color='white', width=800, height=400).generate_from_frequencies(data)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b9ee6",
   "metadata": {},
   "source": [
    "# Prepping for modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e59d8",
   "metadata": {},
   "source": [
    "## Creating three models to pick the best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d3552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84070235",
   "metadata": {},
   "source": [
    "### Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bccbde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df.cleaned)\n",
    "y = df.language_group\n",
    "\n",
    "train_validate, X_test, y_train_validate, y_test = train_test_split(X, y, test_size=.2, stratify = y)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(train_validate, y_train_validate, test_size=.2, stratify = y_train_validate)\n",
    "\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "\n",
    "lm = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "train['predicted'] = lm.predict(X_train)\n",
    "validate['predicted'] = lm.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3fda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(train.actual, train.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(train.predicted, train.actual))\n",
    "print('---')\n",
    "print(classification_report(train.actual, train.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dce678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2%}'.format(accuracy_score(validate.actual, validate.predicted)))\n",
    "print('---')\n",
    "print('Confusion Matrix')\n",
    "print(pd.crosstab(validate.predicted, validate.actual))\n",
    "print('---')\n",
    "print(classification_report(validate.actual, validate.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b180487",
   "metadata": {},
   "source": [
    "# <span style = 'color:green'>KNN: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c1ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df.cleaned)\n",
    "y = df.language_group\n",
    "\n",
    "train_validate, X_test, y_train_validate, y_test = train_test_split(X, y, test_size=.2, stratify = y)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(train_validate, y_train_validate, test_size=.2, stratify = y_train_validate)\n",
    "\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 19, weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "train['knn_predicted'] = knn.predict(X_train)\n",
    "validate['knn_predicted'] = knn.predict(X_validate)\n",
    "\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c23a3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ce187",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f93d3",
   "metadata": {},
   "source": [
    "# Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d344a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(df.cleaned)\n",
    "y = df.language_group\n",
    "\n",
    "train_validate, X_test, y_train_validate, y_test = train_test_split(X, y, test_size=.2, stratify = y)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(train_validate, y_train_validate, test_size=.2, stratify = y_train_validate)\n",
    "\n",
    "train = pd.DataFrame(dict(actual=y_train))\n",
    "validate = pd.DataFrame(dict(actual=y_validate))\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=17, random_state=123)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "train['clf_predicted'] = clf.predict(X_train)\n",
    "validate['clf_predicted'] = clf.predict(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9623b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Decision Tree shows an ~88 percent accuracy on the train set\n",
    "print(classification_report(y_train, train.clf_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc705e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_validate, validate.clf_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
